{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we look at deep reinforcement learning and implement a deep Q-learning agent. As an exercise, you can try implementing an actor-critic agent.\n",
    "\n",
    "# Deep Q-learning\n",
    "\n",
    "Implementing a deep Q-learning agent is quite straightforward. We just need to define a neural network model that will predict the values of the Q function. Then, we need to remember the agent's experiences in a replay buffer and use them to fit the network weights after each simulation of the game. For that, we can use the common `fit` method with mean squared error loss to train the network. We just need to compute the target value correctly (according to the Bellmann equations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "0 15.0\n",
      "1 11.0\n",
      "2 12.0\n",
      "3 25.0\n",
      "4 14.0\n",
      "5 12.0\n",
      "6 16.0\n",
      "7 11.0\n",
      "8 21.0\n",
      "9 18.0\n",
      "10 14.0\n",
      "11 18.0\n",
      "12 26.0\n",
      "13 34.0\n",
      "14 38.0\n",
      "15 14.0\n",
      "16 28.0\n",
      "17 22.0\n",
      "18 44.0\n",
      "19 15.0\n",
      "20 15.0\n",
      "21 25.0\n",
      "22 19.0\n",
      "23 14.0\n",
      "24 13.0\n",
      "25 13.0\n",
      "26 32.0\n",
      "27 12.0\n",
      "28 15.0\n",
      "29 41.0\n",
      "30 11.0\n",
      "31 16.0\n",
      "32 15.0\n",
      "33 17.0\n",
      "34 15.0\n",
      "35 21.0\n",
      "36 17.0\n",
      "37 10.0\n",
      "38 10.0\n",
      "39 17.0\n",
      "40 31.0\n",
      "41 27.0\n",
      "42 11.0\n",
      "43 20.0\n",
      "44 10.0\n",
      "45 20.0\n",
      "46 26.0\n",
      "47 19.0\n",
      "48 19.0\n",
      "49 14.0\n",
      "50 24.0\n",
      "51 10.0\n",
      "52 9.0\n",
      "53 21.0\n",
      "54 16.0\n",
      "55 23.0\n",
      "56 13.0\n",
      "57 34.0\n",
      "58 44.0\n",
      "59 10.0\n",
      "60 9.0\n",
      "61 48.0\n",
      "62 17.0\n",
      "63 27.0\n",
      "64 29.0\n",
      "65 22.0\n",
      "66 42.0\n",
      "67 30.0\n",
      "68 31.0\n",
      "69 69.0\n",
      "70 17.0\n",
      "71 19.0\n",
      "72 30.0\n",
      "73 39.0\n",
      "74 16.0\n",
      "75 19.0\n",
      "76 26.0\n",
      "77 49.0\n",
      "78 59.0\n",
      "79 34.0\n",
      "80 19.0\n",
      "81 19.0\n",
      "82 57.0\n",
      "83 83.0\n",
      "84 46.0\n",
      "85 11.0\n",
      "86 40.0\n",
      "87 44.0\n",
      "88 34.0\n",
      "89 19.0\n",
      "90 41.0\n",
      "91 31.0\n",
      "92 51.0\n",
      "93 24.0\n",
      "94 44.0\n",
      "95 57.0\n",
      "96 24.0\n",
      "97 56.0\n",
      "98 28.0\n",
      "99 39.0\n",
      "100 25.0\n",
      "101 13.0\n",
      "102 15.0\n",
      "103 30.0\n",
      "104 31.0\n",
      "105 36.0\n",
      "106 29.0\n",
      "107 41.0\n",
      "108 15.0\n",
      "109 43.0\n",
      "110 38.0\n",
      "111 64.0\n",
      "112 33.0\n",
      "113 15.0\n",
      "114 41.0\n",
      "115 34.0\n",
      "116 52.0\n",
      "117 15.0\n",
      "118 39.0\n",
      "119 59.0\n",
      "120 30.0\n",
      "121 44.0\n",
      "122 43.0\n",
      "123 14.0\n",
      "124 20.0\n",
      "125 54.0\n",
      "126 28.0\n",
      "127 53.0\n",
      "128 52.0\n",
      "129 79.0\n",
      "130 51.0\n",
      "131 55.0\n",
      "132 66.0\n",
      "133 40.0\n",
      "134 55.0\n",
      "135 43.0\n",
      "136 83.0\n",
      "137 37.0\n",
      "138 128.0\n",
      "139 74.0\n",
      "140 120.0\n",
      "141 94.0\n",
      "142 115.0\n",
      "143 39.0\n",
      "144 87.0\n",
      "145 50.0\n",
      "146 19.0\n",
      "147 92.0\n",
      "148 96.0\n",
      "149 50.0\n",
      "150 41.0\n",
      "151 45.0\n",
      "152 113.0\n",
      "153 193.0\n",
      "154 64.0\n",
      "155 76.0\n",
      "156 100.0\n",
      "157 61.0\n",
      "158 118.0\n",
      "159 103.0\n",
      "160 40.0\n",
      "161 187.0\n",
      "162 178.0\n",
      "163 124.0\n",
      "164 36.0\n",
      "165 163.0\n",
      "166 38.0\n",
      "167 126.0\n",
      "168 273.0\n",
      "169 210.0\n",
      "170 48.0\n",
      "171 50.0\n",
      "172 114.0\n",
      "173 152.0\n",
      "174 186.0\n",
      "175 71.0\n",
      "176 99.0\n",
      "177 122.0\n",
      "178 53.0\n",
      "179 143.0\n",
      "180 93.0\n",
      "181 66.0\n",
      "182 75.0\n",
      "183 129.0\n",
      "184 111.0\n",
      "185 34.0\n",
      "186 62.0\n",
      "187 107.0\n",
      "188 45.0\n",
      "189 148.0\n",
      "190 80.0\n",
      "191 203.0\n",
      "192 165.0\n",
      "193 29.0\n",
      "194 91.0\n",
      "195 230.0\n",
      "196 82.0\n",
      "197 164.0\n",
      "198 61.0\n",
      "199 106.0\n",
      "200 78.0\n",
      "201 71.0\n",
      "202 48.0\n",
      "203 118.0\n",
      "204 57.0\n",
      "205 135.0\n",
      "206 135.0\n",
      "207 96.0\n",
      "208 47.0\n",
      "209 148.0\n",
      "210 43.0\n",
      "211 64.0\n",
      "212 69.0\n",
      "213 178.0\n",
      "214 137.0\n",
      "215 128.0\n",
      "216 75.0\n",
      "217 140.0\n",
      "218 184.0\n",
      "219 180.0\n",
      "220 83.0\n",
      "221 115.0\n",
      "222 81.0\n",
      "223 98.0\n",
      "224 87.0\n",
      "225 131.0\n",
      "226 142.0\n",
      "227 73.0\n",
      "228 100.0\n",
      "229 58.0\n",
      "230 73.0\n",
      "231 181.0\n",
      "232 145.0\n",
      "233 38.0\n",
      "234 136.0\n",
      "235 119.0\n",
      "236 49.0\n",
      "237 88.0\n",
      "238 162.0\n",
      "239 144.0\n",
      "240 237.0\n",
      "241 162.0\n",
      "242 188.0\n",
      "243 88.0\n",
      "244 167.0\n",
      "245 124.0\n",
      "246 138.0\n",
      "247 42.0\n",
      "248 95.0\n",
      "249 78.0\n",
      "250 168.0\n",
      "251 102.0\n",
      "252 124.0\n",
      "253 125.0\n",
      "254 67.0\n",
      "255 91.0\n",
      "256 32.0\n",
      "257 90.0\n",
      "258 31.0\n",
      "259 116.0\n",
      "260 76.0\n",
      "261 60.0\n",
      "262 72.0\n",
      "263 69.0\n",
      "264 127.0\n",
      "265 155.0\n",
      "266 48.0\n",
      "267 192.0\n",
      "268 84.0\n",
      "269 174.0\n",
      "270 98.0\n",
      "271 32.0\n",
      "272 83.0\n",
      "273 135.0\n",
      "274 88.0\n",
      "275 107.0\n",
      "276 87.0\n",
      "277 79.0\n",
      "278 117.0\n",
      "279 113.0\n",
      "280 123.0\n",
      "281 126.0\n",
      "282 89.0\n",
      "283 140.0\n",
      "284 92.0\n",
      "285 124.0\n",
      "286 108.0\n",
      "287 64.0\n",
      "288 60.0\n",
      "289 14.0\n",
      "290 60.0\n",
      "291 109.0\n",
      "292 124.0\n",
      "293 171.0\n",
      "294 192.0\n",
      "295 262.0\n",
      "296 170.0\n",
      "297 168.0\n",
      "298 144.0\n",
      "299 220.0\n",
      "300 243.0\n",
      "301 186.0\n",
      "302 67.0\n",
      "303 267.0\n",
      "304 102.0\n",
      "305 228.0\n",
      "306 145.0\n",
      "307 255.0\n",
      "308 50.0\n",
      "309 161.0\n",
      "310 97.0\n",
      "311 44.0\n",
      "312 205.0\n",
      "313 265.0\n",
      "314 157.0\n",
      "315 235.0\n",
      "316 194.0\n",
      "317 147.0\n",
      "318 195.0\n",
      "319 169.0\n",
      "320 171.0\n",
      "321 103.0\n",
      "322 153.0\n",
      "323 219.0\n",
      "324 160.0\n",
      "325 210.0\n",
      "326 150.0\n",
      "327 216.0\n",
      "328 139.0\n",
      "329 133.0\n",
      "330 131.0\n",
      "331 184.0\n",
      "332 152.0\n",
      "333 158.0\n",
      "334 228.0\n",
      "335 134.0\n",
      "336 185.0\n",
      "337 278.0\n",
      "338 213.0\n",
      "339 363.0\n",
      "340 163.0\n",
      "341 17.0\n",
      "342 237.0\n",
      "343 266.0\n",
      "344 145.0\n",
      "345 203.0\n",
      "346 184.0\n",
      "347 144.0\n",
      "348 118.0\n",
      "349 146.0\n",
      "350 186.0\n",
      "351 236.0\n",
      "352 261.0\n",
      "353 288.0\n",
      "354 178.0\n",
      "355 172.0\n",
      "356 223.0\n",
      "357 90.0\n",
      "358 130.0\n",
      "359 147.0\n",
      "360 191.0\n",
      "361 425.0\n",
      "362 228.0\n",
      "363 149.0\n",
      "364 180.0\n",
      "365 400.0\n",
      "366 279.0\n",
      "367 188.0\n",
      "368 132.0\n",
      "369 181.0\n",
      "370 155.0\n",
      "371 132.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m#print((old_state, action, r, obs, done or terminated))\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     agent\u001b[38;5;241m.\u001b[39mrecord_experience((old_state, action, r, obs, done \u001b[38;5;129;01mor\u001b[39;00m terminated))\n\u001b[1;32m---> 97\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(R)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(i, R)\n",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m d:\n\u001b[0;32m     64\u001b[0m             pred[i][a] \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mamax(next_pred[i])\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# decrease epsilon for the epsilon-greedy strategy\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.01\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\keras\\engine\\training.py:1548\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1541\u001b[0m (\n\u001b[0;32m   1542\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_epoch,\n\u001b[0;32m   1543\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step,\n\u001b[0;32m   1544\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_counters_from_ckpt(\n\u001b[0;32m   1545\u001b[0m     steps_per_epoch_inferred, initial_epoch\n\u001b[0;32m   1546\u001b[0m )\n\u001b[0;32m   1547\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m   1550\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\keras\\engine\\data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[0;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\piaen\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3407\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3408\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3409\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3411\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loosely based on https://keon.io/deep-q-learning/\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs, batch_size = 32, num_batches = 16):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.eps = 1.0\n",
    "        self.eps_decay = 0.995\n",
    "        self.gamma = 0.95\n",
    "        self.exp_buffer = []\n",
    "        self.build_model()\n",
    "    \n",
    "    # vytvari model Q-site\n",
    "    def build_model(self):\n",
    "        self.model = tf.keras.models.Sequential([tf.keras.layers.Dense(24, input_dim=self.num_inputs),\n",
    "                                                 tf.keras.layers.ReLU(),\n",
    "                                                 tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "                                                 tf.keras.layers.ReLU(),\n",
    "                                                 tf.keras.layers.Dense(self.num_outputs, activation='linear')])\n",
    "        opt = tf.keras.optimizers.Adam(lr=0.001)\n",
    "        self.model.compile(optimizer=opt, loss='mse', run_eagerly=True)\n",
    "        \n",
    "    # returns agent's action - epsilon greedy when training and the best one otherwise\n",
    "    def action(self, state, train=False):\n",
    "        if train and np.random.uniform() < self.eps:\n",
    "            return np.random.randint(self.num_outputs)\n",
    "        else: \n",
    "            return np.argmax(self.model.predict(state, verbose=False)[0])\n",
    "        \n",
    "    # save experience to buffer\n",
    "    def record_experience(self, exp):\n",
    "        self.exp_buffer.append(exp)\n",
    "        if len(self.exp_buffer) > 2000:\n",
    "            self.exp_buffer = self.exp_buffer[-2000:]\n",
    "    \n",
    "    # train based on buffer\n",
    "    def train(self):\n",
    "        import pprint\n",
    "        if (len(self.exp_buffer) <= self.batch_size):\n",
    "            return\n",
    "        \n",
    "        for _ in range(self.num_batches):\n",
    "            batch = random.sample(self.exp_buffer, self.batch_size)\n",
    "            #pprint.pprint(batch)\n",
    "            states = np.array([s for (s, _, _, _, _) in batch])\n",
    "            next_states = np.array([ns for (_, _, _, ns, _) in batch])\n",
    "            states = states.reshape((-1, self.num_inputs))\n",
    "            next_states = next_states.reshape((-1, self.num_inputs))\n",
    "            pred = self.model.predict(states, verbose=False)\n",
    "            next_pred = self.model.predict(next_states, verbose=False)\n",
    "            # compute the target values\n",
    "            for i, (s, a, r, ns, d) in enumerate(batch):\n",
    "                pred[i][a] = r\n",
    "                if not d:\n",
    "                    pred[i][a] = r + self.gamma*np.amax(next_pred[i])\n",
    "\n",
    "            self.model.fit(states, pred, epochs=1, verbose=False)\n",
    "        # decrease epsilon for the epsilon-greedy strategy\n",
    "        if self.eps > 0.01:\n",
    "            self.eps = self.eps*self.eps_decay\n",
    "\n",
    "# create agent (4 inputs, 2 actions)\n",
    "agent = DQNAgent(4, 2)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "\n",
    "# train the network on 1000 runs of the environment\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    obs = np.reshape(obs, newshape=(1, -1))\n",
    "    done = False\n",
    "    terminated = False\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while not (done or terminated):\n",
    "        old_state = obs\n",
    "        action = agent.action(obs, train=True)\n",
    "        obs, r, done, terminated, _ = env.step(action)\n",
    "        #print(obs, r, done, terminated)\n",
    "        R += r\n",
    "        t += 1\n",
    "        r = r if not (done or terminated) else 10\n",
    "        obs = np.reshape(obs, newshape=(1, -1))\n",
    "        #print((old_state, action, r, obs, done or terminated))\n",
    "        agent.record_experience((old_state, action, r, obs, done or terminated))\n",
    "    agent.train()\n",
    "    \n",
    "    rewards.append(R)\n",
    "    print(i, R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test, how well the agent solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "187.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs = np.reshape(obs, newshape=(1, -1))\n",
    "done = False\n",
    "R = 0\n",
    "t = 0\n",
    "while not (done or terminated):\n",
    "    old_state = obs\n",
    "    action = agent.action(obs, train=False)\n",
    "    obs, r, done, terminated, _ = env.step(action)\n",
    "    obs = np.reshape(obs, newshape=(1, -1))\n",
    "    R += r\n",
    "    t += 1\n",
    "        \n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (not a homework)\n",
    "\n",
    "Choose one of the problems in OpenAI gym with continuous actions and try to solve it with the actor-critic approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
